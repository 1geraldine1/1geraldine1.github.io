---
title: "seq2seq 모델 작성중"
date: 2022-06-14T18:08:30-04:00
categories:
  - '2022-06 TIL'
tags:
  - '20220614'
  - 'TIL'
  - 'seq2seq'
---

# 개요

* [FormNet에 관한 글을 번역하며 읽어가던 중](https://1geraldine1.github.io/2022-06%20til/%EA%B5%AC%EA%B8%80-FormNet%EA%B3%BC-%EC%9D%B4%ED%95%B4%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B8%B0%EB%B0%98-%EC%A7%80%EC%8B%9D-%EC%8A%B5%EB%93%9D-1/), 다음 단락인 ETC에 관한 부분에서 지식의 한계가 찾아왔다.

* ETC(Extended Transformer Construction)는 Transformer 모델에서 개념을 확장한 버전이다.

* 이참에 연관된 개념들을 같이 정리해보기로 했다.

# 1. seq2seq

## 1-1. 정의

* 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델의 통칭.

  * 입력 시퀀스를 질문, 출력 시퀀스를 답변으로 구성한다면 질문에 따른 답변을 출력하는 챗봇이 나온다.

  * 입력 시퀀스를 입력 문장, 출력 시퀀스를 번역 문장으로 구성한다면 입력된 문장을 번역하는 번역기가 나온다.

## 1-2. 구조

* 인코더(Encoder)와 디코더(Decoder) 두개의 모듈로 구성된다.

* 인코더는 입력된 시퀀스를 순차적으로 입력받아 모든 정보를 압축하여 하나의 벡터로 만든다.

  * 이를 ```컨텍스트 벡터(context vector)```라고 한다.

* 기본적으로 인코더와 디코더 모두 RNN 아키텍처로 구성된다.

  * 라고는 하는데, 성능 문제로 인해 LSTM<sup>[[1]](#footnote_1)</sup> 또는 GRU<sup>[[1]](#footnote_1)</sup> 셀들로 구성된다고 한다.




* GCN<sup>[[1]](#footnote_1)</sup>을 통해 인접한 토큰에 대해 의미있는 정보에 해당하는 Super-Tokens를 구성한다.

### 세부 작동방식 - FormNet




## Transformer 모델의 개념 및 이해

### 

# 참고한 글

* [딥러닝을 이용한 자연어처리 입문](https://wikidocs.net/24996)

----

<a name="footnote_1">[1]</a> : 개념에 관해서는 이전에 작성한 [LSTM과 GRU에 대한 이해](https://1geraldine1.github.io/2022-06%20til/LSTM%EA%B3%BC-GRU%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4/)글을 참고.   
<a name="footnote_2">[2]</a> :  

