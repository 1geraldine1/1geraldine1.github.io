---
title: "LSTM과 GRU에 대한 이해"
date: 2022-06-13T18:08:30-04:00
categories:
  - '2022-06 TIL'
tags:
  - '20220613'
  - 'TIL'
  - 'RNN'
  - 'LSTM'
  - 'GRU'
---

# 개요

* seq2seq 글을 작성하던 중, LSTM과 GRU에 대한 설명글이 다소 길어질 것 같기도 하고, 중요한 개념이므로 별도의 문서를 생성하는쪽이 나을것같아 글을 분리했다.

# RNN

* 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력 레이어로 보내면서, <U>동시에 다음 은닉층 노드의 입력으로도 보낸다.</U>

* 수식으로 표현하면, $h_t = tanh(h_{t-1}W_{h} + x_tW_x + b)$

    * $h_t$는 t시점의 출력벡터
    * $W_h$는 이전 은닉층에서부터 보내진 가중치
    * $W_x$는 h번째 x를 은닉층으로 보내는 가중치
    * $b$는 각 편향(bias)을 단순화한 값

* 이전 계산에 사용된 값 역시 다음 계산에 반영되므로 문맥(시퀀스)을 활용한 예측이 가능해진다.

* 또한, <U>입력과 출력의 길이를 다르게 설정할수 있으므로</U>, 다양한 분야에 활용할수 있다.

  * <B>일대다 구조(one-to-many)</B> : <U>하나의 사진 입력에 대해 사진의 설명을 출력하는 <B>이미지 캡셔닝</B></U>. 사진의 설명들은 단어의 나열이므로, 시퀀스 출력이다.

  * <B>다대일 구조(many-to-one)</B> : <U>문장을 이루는 단어들을 입력받고, 감성을 분류하는 <B>감성 분류</B></U> 또는 <U>메일 내용을 입력받아 스팸 여부를 분류하는 <B>스팸 메일 분류</B></U>.

  * <B>다대다 구조(many-to-many)</B> : <U>문장을 이루는 여러 단어를 입력받아 대응하는 외국어를 출력하는 <B>기계 번역</B></U> 등.

* 다만, 사용하는 정보와 그 정보를 사용하는 시점간의 거리가 멀어질수록(시간 격차가 커질수록) 역전파시 그래디언트<sup>[[1]](#footnote_1)</sup>가 줄어드는 현상이 발생해 학습 능력이 크게 저하된다.

  * 이러한 문제를 <U>Long-term dependency</U>라고 한다.


# LSTM

* RNN의 단점을 극복하고자 고안된것이 바로 LSTM이다.

## cell state

* cell state라 불리는, 처음의 정보를 계속 유지하는 체인에 정보를 더하거나 없애는 방식으로 가중치를 변경한다.

* cell state에서 정보를 버리거나, 보존할때 게이트에 들어온 값들을 토대로 결정하게 된다.

  ![cell state](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile10.uf.tistory.com%2Fimage%2F99CB87505ACB86A00FAB6F)

## forget gate layer

* cell state의 정보중 어떤것을 버리고 보존할지 결정하는 sigmoid layer이다.

* $h_{t-1}$과 $x_t$를 받아 0과 1사이의 값을 보내주는데, 이 값이 <U>1이면 "모든 정보를 보존", 0이면 "모든 정보를 파기"</U>하는 방식으로 작동한다.

  ![forget gate layer](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile2.uf.tistory.com%2Fimage%2F9957DB445ACB86A02155EA)

## input gate layer

* 앞으로 들어오는 정보중 어떤것을 cell state에 더할지 정하는 레이어.

* sigmoid layer를 통해 어떤 값을 업데이트할지 정하고($i_t$), tanh layer가 새로운 후보값인 $\bar{C_t}$벡터를 만들어 cell state에 더할 준비를 한다.

  ![input gate layer](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile4.uf.tistory.com%2Fimage%2F99D969495ACB86A00BFC15)

## cell state update

* 과거의 정보인 $C_{t-1}$를 업데이트해 $C_t$로 만드는 과정.

* 이전 state인 $C_{t-1}$에 forget gate layer에서 나온 결과값인 $f_t$를 곱해 버리기로 한 정보를 제거한다.

* 그 다음, $i_t * \bar{C_t}$를 더하는데, 이것은 input gate layer에서 정한 후보값을 $i_t$로 scaling한 값이다.

  ![cell state update](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile9.uf.tistory.com%2Fimage%2F997589405ACB86A00CADEA)

## output gate layer

* 위에서 업데이트한 cell state에서 무엇을 output으로 내보낼지 정하는 레이어.

* sigmoid에 input data를 태워 cell state의 어느 부분을 output으로 내보낼지 결정한다.

* cell state를 tanh에 태워 -1~1 사이의 값을 받은후, sigmoid에서 받은 결과값과 곱해주면 output으로 내보낼 값들만 내보낼수 있게 된다.

  ![output gate layer](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F99FB824C5ACB86A10D4182)

# GRU

* Gated Recurrent Unit

* LSTM 구조를 변형한 모델이다.

* LSTM은 복잡한 구조로 인해 RNN에 비해 파라미터가 많이 필요했고, 데이터가 충분하지 않은경우 오버피팅이 발생하게 된다.

* 이러한 단점을 개선하기 위해 등장한것이 GRU다.

## LSTM과 GRU의 차이점

1. <U>LSTM의 forget gate와 input gate를 하나로 통합한 update gate</U>를 만들고, <U>output gate를 reset gate로 대체</U>한다.

2. <U>Cell state와 Hidden state를 통합하여 하나의 벡터($h(t)$)로 표현</U>한다.

3. LSTM에서는 forget과 input이 독립적이었으나, GRU에서는 <U>정보의 총량이 정해져있고, forget한만큼 input하는 방식으로 제어한다.</U>

* LSTM과 성능은 비슷하지만, 간단해진 구조로 인해 파라미터 갯수가 감소하고, 연산 비용이 줄어들었다.

![LSTM과 GRU의 차이점](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb6kVlm%2Fbtq4Cs5VOPX%2Fr73EUarFNRgaDGdJ7r1Hvk%2Fimg.png) https://excelsior-cjh.tistory.com/185

## GRU 진행 단계

### Gate

![Gate](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FpTuSe%2Fbtq4reaQC57%2FGu9QxFYfUHOTt6rFvj9Yw1%2Fimg.png)

1. <B>Update Gate</B>, $z(t)$
    
    * 이전의 정보를 얼마나 유지할지 결정함.
    * LSTM에서의 forget gate, input gate 역할.
    * <B>Long-term dependency</B>를 의미한다.
 
2. <B>Reset Gate</B> , $r(t)$

    * 이전의 정보를 얼마나 잊어야할지 결정함.
    * LSTM에서의 output gate 역할.
    * <B>Short-term dependency</B>를 의미한다.

$z_t = \sigma(W^{(z)}x_t + U^{(z)}h_{t-1})$

$r_t = \sigma(W^{(r)}x_t + U^{(r)}h_{t-1})$ 

### Candidate Hidden State

* LSTM에서의 cell state update와 유사한 부분.

* reset gate ($r(t)$)와 $Uh_{t-1}$을 곱해, 이전까지 가지고 있던 정보중에서 어떤 정보를 지울지 결정함.

* update gate의 값을 반영하지 않았으므로, 아직은 <U>업데이트 값 후보군</U>의 의미로 'Candidate' Hidden State다.

* <U>reset gate를 모두 1, update gate를 모두 0으로 설정한다면, 기존의 RNN과 같아진다</U>

![Candidate Hidden State](https://d2l.ai/_images/gru-2.svg)

* $\tilde{h_t} = tanh(Wx_t + r \circ Uh_{t-1})$

### Hidden State

* update gate를 Candidate Hidden State를 합쳐만든 hidden state($h_t$)와 이전의 hidden state($h_{t-1}$)을 어느정도씩 사용할지 결정한다.

* $z(t)$함수가 0이라면 old state인 $h_{t-1}$을 그대로 가져가고, 1이라면 candidate state인 $\tilde{h}_t$로 완전히 대체된다.

* $h_t = z_t \circ h_{t-1} + (1-z_t) \circ \tilde{h}_t$






# 참고한 글

* [dgkim5360님의 LSTM 이해하기](https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr)

* [딥러닝을 이용한 자연어처리 입문](https://wikidocs.net/22886)

* [yuns_u님의 RNN에 관한 글](https://velog.io/@yuns_u/%EC%88%9C%ED%99%98-%EC%8B%A0%EA%B2%BD%EB%A7%9DRNN-Recurrent-Neural-Network)

* [ratsgo님의 RNN, LSTM에 관한 글](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)

* [hyen4110님의 GRU에 관한 글](https://hyen4110.tistory.com/26)

* [DIVE INTO DEEP LEARNING - GRU](https://d2l.ai/chapter_recurrent-modern/gru.html)


------------------  
<a name="footnote_1">[1]</a> : 역전파 과정에서 RNN함수의 활성화 함수인 tanh 함수의 미분값을 전달하게 되는데, tanh를 미분한 함수는 0~1 사이의 값을 가진다. 

그런데, 이 미분값을 역전파하는 노드마다 곱해주어야 하는데, 예를 들어 0.9의 미분값을 노드 10개에 곱해주면 0.349 정도로 감소하게 되고, 시퀀스 앞쪽의 은닉층에는 거의 역전파 정보가 전해지지 않게 된다.  

이렇게 역전파로 전해져야할 tanh의 미분값 그래프의 기울기가 사라진다고 표현하므로, 이러한 문제를 <U>기울기 소실(vanishing gradient problem)</U>이라 부른다. 

long-term dependency의 원인이 vanishing gradient라고 기억해두면 될것같다.
